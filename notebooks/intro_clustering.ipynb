{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Clusterización\n",
    "\n",
    "La clusterización (clustering) es una técnica de **aprendizaje no supervisado** que agrupa datos similares en conjuntos llamados clusters. A diferencia del aprendizaje supervisado, no tenemos etiquetas predefinidas.\n",
    "\n",
    "## ¿Para qué sirve?\n",
    "\n",
    "- **Segmentación de clientes**: Agrupar clientes con comportamientos similares\n",
    "- **Detección de anomalías**: Identificar datos que no pertenecen a ningún grupo\n",
    "- **Compresión de imágenes**: Reducir colores agrupando píxeles similares\n",
    "- **Organización de documentos**: Agrupar textos por temas\n",
    "\n",
    "## Algoritmos que veremos\n",
    "\n",
    "1. **K-Means**: El más popular, agrupa por distancia a centroides\n",
    "2. **DBSCAN**: Clustering basado en densidad, detecta formas irregulares\n",
    "3. **Clustering Jerárquico**: Crea una jerarquía de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ejemplo 1: K-Means - Datos Sintéticos\n",
    "\n",
    "K-Means es un algoritmo iterativo que:\n",
    "1. Inicializa K centroides aleatoriamente\n",
    "2. Asigna cada punto al centroide más cercano\n",
    "3. Recalcula los centroides como el promedio de los puntos asignados\n",
    "4. Repite hasta convergencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización del Proceso Iterativo de K-Means\n",
    "\n",
    "Antes de aplicar K-Means con scikit-learn, veamos cómo funciona el algoritmo paso a paso con k=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos simples para visualización\n",
    "np.random.seed(42)\n",
    "X_demo, _ = make_blobs(n_samples=150, centers=2, n_features=2, \n",
    "                       cluster_std=1.2, random_state=42)\n",
    "\n",
    "# Función para calcular distancias euclidianas\n",
    "def calcular_distancias(X, centroides):\n",
    "    \"\"\"Calcula la distancia de cada punto a cada centroide\"\"\"\n",
    "    distancias = np.zeros((X.shape[0], centroides.shape[0]))\n",
    "    for i, centroide in enumerate(centroides):\n",
    "        distancias[:, i] = np.sqrt(np.sum((X - centroide)**2, axis=1))\n",
    "    return distancias\n",
    "\n",
    "# Función para asignar clusters\n",
    "def asignar_clusters(X, centroides):\n",
    "    \"\"\"Asigna cada punto al centroide más cercano\"\"\"\n",
    "    distancias = calcular_distancias(X, centroides)\n",
    "    return np.argmin(distancias, axis=1)\n",
    "\n",
    "# Función para actualizar centroides\n",
    "def actualizar_centroides(X, labels, k):\n",
    "    \"\"\"Calcula los nuevos centroides como el promedio de cada cluster\"\"\"\n",
    "    centroides = np.zeros((k, X.shape[1]))\n",
    "    for i in range(k):\n",
    "        puntos_cluster = X[labels == i]\n",
    "        if len(puntos_cluster) > 0:\n",
    "            centroides[i] = puntos_cluster.mean(axis=0)\n",
    "    return centroides\n",
    "\n",
    "# Implementación manual de K-Means con k=2\n",
    "k = 2\n",
    "max_iters = 6\n",
    "\n",
    "# Inicializar centroides aleatoriamente\n",
    "indices_iniciales = np.random.choice(X_demo.shape[0], k, replace=False)\n",
    "centroides = X_demo[indices_iniciales]\n",
    "\n",
    "# Almacenar el historial\n",
    "historial_centroides = [centroides.copy()]\n",
    "historial_labels = []\n",
    "\n",
    "# Algoritmo iterativo\n",
    "for iteracion in range(max_iters):\n",
    "    # Paso 1: Asignar cada punto al centroide más cercano\n",
    "    labels = asignar_clusters(X_demo, centroides)\n",
    "    historial_labels.append(labels.copy())\n",
    "    \n",
    "    # Paso 2: Actualizar centroides\n",
    "    nuevos_centroides = actualizar_centroides(X_demo, labels, k)\n",
    "    \n",
    "    # Verificar convergencia\n",
    "    if np.allclose(centroides, nuevos_centroides):\n",
    "        print(f\"Convergencia alcanzada en la iteración {iteracion + 1}\")\n",
    "        centroides = nuevos_centroides\n",
    "        historial_centroides.append(centroides.copy())\n",
    "        break\n",
    "    \n",
    "    centroides = nuevos_centroides\n",
    "    historial_centroides.append(centroides.copy())\n",
    "\n",
    "print(f\"Número total de iteraciones: {len(historial_labels)}\")\n",
    "\n",
    "# Visualizar cada iteración\n",
    "n_plots = len(historial_labels)\n",
    "n_cols = 3\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "axes = axes.flatten() if n_plots > 1 else [axes]\n",
    "\n",
    "colores = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "for idx, (labels, centroides_iter) in enumerate(zip(historial_labels, historial_centroides[:-1])):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Graficar puntos coloreados por cluster\n",
    "    for cluster_id in range(k):\n",
    "        mask = labels == cluster_id\n",
    "        ax.scatter(X_demo[mask, 0], X_demo[mask, 1], \n",
    "                  c=colores[cluster_id], alpha=0.6, s=50, \n",
    "                  label=f'Cluster {cluster_id}')\n",
    "    \n",
    "    # Graficar centroides\n",
    "    ax.scatter(centroides_iter[:, 0], centroides_iter[:, 1], \n",
    "              c='red', marker='X', s=400, edgecolors='black', \n",
    "              linewidths=2, label='Centroides', zorder=5)\n",
    "    \n",
    "    # Graficar líneas desde centroides anteriores (si existe)\n",
    "    if idx > 0:\n",
    "        centroides_anterior = historial_centroides[idx - 1]\n",
    "        for i in range(k):\n",
    "            ax.plot([centroides_anterior[i, 0], centroides_iter[i, 0]],\n",
    "                   [centroides_anterior[i, 1], centroides_iter[i, 1]],\n",
    "                   'k--', alpha=0.5, linewidth=1.5)\n",
    "    \n",
    "    ax.set_title(f'Iteración {idx + 1}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Ocultar ejes sobrantes\n",
    "for idx in range(n_plots, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar centroides finales\n",
    "print(\"\\nCentroides finales:\")\n",
    "print(historial_centroides[-1])\n",
    "\n",
    "# Calcular inercia final\n",
    "labels_final = historial_labels[-1]\n",
    "inercia = 0\n",
    "for i in range(k):\n",
    "    puntos_cluster = X_demo[labels_final == i]\n",
    "    inercia += np.sum((puntos_cluster - historial_centroides[-1][i])**2)\n",
    "print(f\"\\nInercia final: {inercia:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observaciones clave:**\n",
    "\n",
    "- **Iteración 1**: Los centroides se inicializan en posiciones aleatorias\n",
    "- **Iteraciones siguientes**: Los centroides se mueven (líneas punteadas) hacia el centro de los puntos asignados\n",
    "- **Convergencia**: El algoritmo termina cuando los centroides ya no se mueven significativamente\n",
    "- **Resultado final**: Los centroides representan el \"centro\" de cada cluster\n",
    "\n",
    "Este proceso demuestra por qué K-Means es sensible a la inicialización y puede converger a diferentes soluciones dependiendo de dónde comiencen los centroides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos sintéticos con 4 clusters bien definidos\n",
    "X_blobs, y_true = make_blobs(n_samples=400, centers=4, n_features=2, \n",
    "                              cluster_std=0.8, random_state=42)\n",
    "\n",
    "# Visualizar los datos originales\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], alpha=0.6, s=50)\n",
    "plt.title('Datos sin etiquetar')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_true, cmap='viridis', alpha=0.6, s=50)\n",
    "plt.title('Clusters verdaderos (desconocidos en la práctica)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Forma de los datos: {X_blobs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas de Evaluación de Clustering\n",
    "\n",
    "Como no tenemos etiquetas verdaderas en clustering, necesitamos métricas que evalúen la calidad de los clusters basándose en:\n",
    "1. **Cohesión**: Qué tan compactos son los clusters internamente\n",
    "2. **Separación**: Qué tan separados están los clusters entre sí\n",
    "\n",
    "#### 1. Inercia (Within-Cluster Sum of Squares - WCSS)\n",
    "\n",
    "**Definición matemática:**\n",
    "\n",
    "$$\\text{Inercia} = \\sum_{i=1}^{K} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2$$\n",
    "\n",
    "Donde:\n",
    "- $K$ = número de clusters\n",
    "- $C_i$ = conjunto de puntos en el cluster $i$\n",
    "- $x$ = un punto de datos\n",
    "- $\\mu_i$ = centroide del cluster $i$\n",
    "- $\\|x - \\mu_i\\|^2$ = distancia euclidiana al cuadrado\n",
    "\n",
    "**Interpretación intuitiva:**\n",
    "\n",
    "La inercia mide qué tan compactos son los clusters. Imagina que cada punto está conectado a su centroide por un resorte - la inercia sería la energía total almacenada en todos esos resortes.\n",
    "\n",
    "- **Valor más bajo** = clusters más compactos (mejor)\n",
    "- **Valor más alto** = puntos más dispersos del centroide\n",
    "- **Problema**: Siempre disminuye al aumentar K (con K=n, inercia=0)\n",
    "\n",
    "Por eso usamos el **método del codo**: buscamos el punto donde la inercia deja de disminuir significativamente.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Silhouette Score\n",
    "\n",
    "**Definición matemática:**\n",
    "\n",
    "Para cada punto $i$:\n",
    "\n",
    "$$s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$$\n",
    "\n",
    "Donde:\n",
    "- $a(i)$ = distancia promedio de $i$ a todos los demás puntos en su mismo cluster (cohesión)\n",
    "- $b(i)$ = distancia promedio de $i$ a todos los puntos del cluster más cercano (separación)\n",
    "\n",
    "El Silhouette Score es el promedio de $s(i)$ sobre todos los puntos.\n",
    "\n",
    "**Interpretación intuitiva:**\n",
    "\n",
    "El silhouette score mide qué tan bien está asignado cada punto a su cluster. Para cada punto pregunta:\n",
    "\n",
    "- \"¿Estoy más cerca de los puntos de mi cluster o del cluster vecino?\"\n",
    "\n",
    "**Valores:**\n",
    "- **+1**: El punto está muy bien asignado (lejos de otros clusters)\n",
    "- **0**: El punto está en la frontera entre dos clusters\n",
    "- **-1**: El punto probablemente está en el cluster equivocado\n",
    "\n",
    "**Ventajas:**\n",
    "- Rango acotado [-1, 1], fácil de interpretar\n",
    "- Considera tanto cohesión como separación\n",
    "- No depende del número de clusters\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Davies-Bouldin Index\n",
    "\n",
    "**Definición matemática:**\n",
    "\n",
    "$$DB = \\frac{1}{K} \\sum_{i=1}^{K} \\max_{j \\neq i} \\left( \\frac{s_i + s_j}{d(c_i, c_j)} \\right)$$\n",
    "\n",
    "Donde:\n",
    "- $s_i$ = distancia promedio de todos los puntos en el cluster $i$ a su centroide $c_i$\n",
    "- $d(c_i, c_j)$ = distancia entre los centroides $i$ y $j$\n",
    "\n",
    "**Interpretación intuitiva:**\n",
    "\n",
    "El índice Davies-Bouldin mide la \"similitud\" promedio entre cada cluster y su cluster más parecido. Para cada cluster pregunta:\n",
    "\n",
    "- \"¿Qué tan disperso estoy internamente comparado con qué tan lejos está mi vecino más cercano?\"\n",
    "\n",
    "Piensa en clusters como grupos de personas:\n",
    "- **Numerador grande** ($s_i + s_j$): Los grupos están muy dispersos internamente\n",
    "- **Denominador grande** ($d(c_i, c_j)$): Los centros de los grupos están muy separados\n",
    "- **Queremos**: Grupos compactos (numerador pequeño) y bien separados (denominador grande)\n",
    "\n",
    "**Valores:**\n",
    "- **Valor más bajo** = mejor clustering\n",
    "- **0** = mejor valor posible (clusters perfectamente separados y compactos)\n",
    "- **Sin límite superior**: puede crecer indefinidamente\n",
    "\n",
    "---\n",
    "\n",
    "#### Comparación de las métricas\n",
    "\n",
    "| Métrica | Rango | Mejor valor | Ventaja principal | Desventaja |\n",
    "|---------|-------|-------------|-------------------|------------|\n",
    "| **Inercia** | [0, ∞) | Más bajo | Simple de calcular | Siempre disminuye con más K |\n",
    "| **Silhouette** | [-1, 1] | Cercano a 1 | Intuitivo, considera separación | Costoso computacionalmente |\n",
    "| **Davies-Bouldin** | [0, ∞) | Cercano a 0 | Penaliza clusters mal separados | Asume clusters convexos |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar K-Means con k=4\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "y_kmeans = kmeans.fit_predict(X_blobs)\n",
    "\n",
    "# Visualizar resultados\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_kmeans, cmap='viridis', alpha=0.6, s=50)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "            c='red', marker='X', s=300, edgecolors='black', linewidths=2, label='Centroides')\n",
    "plt.title('Resultados de K-Means')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.colorbar(label='Cluster')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Comparación con clusters verdaderos\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_true, cmap='viridis', alpha=0.6, s=50)\n",
    "plt.title('Clusters Verdaderos')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Métricas de evaluación\n",
    "silhouette = silhouette_score(X_blobs, y_kmeans)\n",
    "davies_bouldin = davies_bouldin_score(X_blobs, y_kmeans)\n",
    "\n",
    "print(f\"\\nMétricas de evaluación:\")\n",
    "print(f\"Silhouette Score: {silhouette:.3f} (rango: [-1, 1], mejor: cercano a 1)\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.3f} (mejor: cercano a 0)\")\n",
    "print(f\"Inercia (suma de distancias al cuadrado): {kmeans.inertia_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo elegir el número de clusters (K)?\n",
    "\n",
    "El **método del codo** nos ayuda a encontrar el K óptimo observando dónde la inercia deja de disminuir significativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método del codo\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(X_blobs)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_blobs, kmeans_temp.labels_))\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Método del codo\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Número de Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inercia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "axes[0].set_title('Método del Codo', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(x=4, color='red', linestyle='--', label='K óptimo = 4')\n",
    "axes[0].legend()\n",
    "\n",
    "# Silhouette Score\n",
    "axes[1].plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Número de Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score vs K', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(x=4, color='red', linestyle='--', label='K óptimo = 4')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservaciones:\")\n",
    "print(\"- El 'codo' está en K=4, donde la inercia empieza a disminuir más lentamente\")\n",
    "print(\"- El Silhouette Score es máximo en K=4, confirmando que es el mejor número de clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ejemplo 2: K-Means con Dataset Real (Iris)\n",
    "\n",
    "Usaremos el famoso dataset Iris para segmentar flores según sus características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset Iris\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris_true = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Crear DataFrame para mejor visualización\n",
    "df_iris = pd.DataFrame(X_iris, columns=feature_names)\n",
    "df_iris['especie'] = pd.Categorical.from_codes(y_iris_true, iris.target_names)\n",
    "\n",
    "print(\"Primeras filas del dataset Iris:\")\n",
    "print(df_iris.head(10))\n",
    "print(f\"\\nForma: {df_iris.shape}\")\n",
    "print(f\"\\nEspecies: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar los datos (importante para K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# Aplicar K-Means con k=3 (sabemos que hay 3 especies)\n",
    "kmeans_iris = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "y_iris_pred = kmeans_iris.fit_predict(X_iris_scaled)\n",
    "\n",
    "# Visualizar usando las dos primeras características\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Clustering predicho\n",
    "axes[0].scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris_pred, cmap='viridis', alpha=0.6, s=100)\n",
    "axes[0].set_xlabel(feature_names[0])\n",
    "axes[0].set_ylabel(feature_names[1])\n",
    "axes[0].set_title('K-Means Clustering')\n",
    "\n",
    "# Especies verdaderas\n",
    "axes[1].scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris_true, cmap='viridis', alpha=0.6, s=100)\n",
    "axes[1].set_xlabel(feature_names[0])\n",
    "axes[1].set_ylabel(feature_names[1])\n",
    "axes[1].set_title('Especies Verdaderas')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluación\n",
    "silhouette_iris = silhouette_score(X_iris_scaled, y_iris_pred)\n",
    "print(f\"\\nSilhouette Score: {silhouette_iris:.3f}\")\n",
    "\n",
    "# Comparar clusters con especies reales\n",
    "df_comparison = pd.DataFrame({\n",
    "    'Especie Real': pd.Categorical.from_codes(y_iris_true, iris.target_names),\n",
    "    'Cluster': y_iris_pred\n",
    "})\n",
    "print(\"\\nTabla de contingencia (Especies vs Clusters):\")\n",
    "print(pd.crosstab(df_comparison['Especie Real'], df_comparison['Cluster']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mineria-datos-itam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
